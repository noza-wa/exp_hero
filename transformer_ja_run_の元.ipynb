{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_ja_run の元",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noza-wa/exp_hero/blob/master/transformer_ja_run_%E3%81%AE%E5%85%83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuH2GuModvwz"
      },
      "source": [
        "# Githubのコピー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6De4lcZdvwy"
      },
      "source": [
        "# Transformerによる日本語データのネガポジ判定と根拠の可視化のコピー　のざわ勉強用\n",
        "\n",
        "\n",
        "- 使用するデータセットはchABSA-datasetです。これは、日本の上場企業の有価証券報告書から文章を取り出し、それがポジテイブな表現なのか、ネガティブな表現なのかをまとめたものです。\n",
        "- 学習後、テスト文章で推論を行い、その文章のどの単語が判断根拠になっているか(Attention)を可視化します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VONS0bsjeKnY",
        "outputId": "66e70d0d-932a-4a15-8330-ce4a39b61790"
      },
      "source": [
        "!git clone https://github.com/cedro3/Transformer.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformer'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 124 (delta 68), reused 32 (delta 13), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (124/124), 25.36 MiB | 19.42 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToazW8JZeL2B",
        "outputId": "3d73dc17-1134-4297-c4fe-eef21e361bec"
      },
      "source": [
        "cd Transformer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tw6J5t2Af2a"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVy18KqcZAsB"
      },
      "source": [
        "# Mecab＋辞書、mojimojiのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48CtVHvJfIVl",
        "outputId": "5cdb80b6-4d3a-44d7-bdcb-5104b39f0990"
      },
      "source": [
        "# 形態素分析ライブラリーMeCab と 辞書(mecab-ipadic-NEologd)のインストール \n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc\n",
        "\n",
        "# 日本語半角・全角変換ライブラリーmojimojiのインストール\n",
        "!pip install mojimoji"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecab-ipadic-neologd'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 75 (delta 5), reused 54 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), done.\n",
            "Collecting mojimoji\n",
            "  Downloading mojimoji-0.0.11-cp37-cp37m-manylinux1_x86_64.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: mojimoji\n",
            "Successfully installed mojimoji-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOfGbf0DZmVl"
      },
      "source": [
        "# 日本語fastTextモデルのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5yV9JNmZwtj"
      },
      "source": [
        "# コード本体"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew7POFI-dvw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd0c6ef-49b2-4149-e4e9-b892b9a10ea5"
      },
      "source": [
        "# パッケージのimport\n",
        "!pip install torch<=1.2.0\n",
        "!pip install torchtext==0.5\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: =1.2.0: No such file or directory\n",
            "Collecting torchtext==0.5\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.5) (3.7.4.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMP_TjGHdvw4"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAybS2FLdvxB"
      },
      "source": [
        "# DatasetとDataLoaderを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "E3ff7GvhdvxC",
        "scrolled": true,
        "outputId": "4fecb194-81c9-4856-8f86-f425b25f1c9f"
      },
      "source": [
        "from dataloader_ja import get_chABSA_DataLoaders_and_TEXT\n",
        "\n",
        "\n",
        "# 読み込み\n",
        "train_dl, val_dl, TEXT = get_chABSA_DataLoaders_and_TEXT(max_length=256, batch_size=8)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8a9fd4a9b1de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_chABSA_DataLoaders_and_TEXT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 辞書オブジェクトにまとめる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Transformer/dataloader_ja.py\u001b[0m in \u001b[0;36mget_chABSA_DataLoaders_and_TEXT\u001b[0;34m(max_length, batch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# torchtextで日本語ベクトルとして日本語学習済みモデルを読み込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mjapanese_fastText_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/model.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# ベクトル化したバージョンのボキャブラリーを作成します\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    370\u001b[0m                             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no vectors found at {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading vectors from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: no vectors found at .vector_cache/./data/model.vec"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BFaQmiQxU1W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a13mmNoDdvxF"
      },
      "source": [
        "# ネットワークモデルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uACKqehvdvxJ"
      },
      "source": [
        "from transformer import TransformerClassification\n",
        "\n",
        "# モデル構築\n",
        "net = TransformerClassification(\n",
        "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
        "\n",
        "# ネットワークの初期化を定義\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        # Liner層の初期化\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "# TransformerBlockモジュールを初期化実行\n",
        "net.net3_1.apply(weights_init)\n",
        "net.net3_2.apply(weights_init)\n",
        "print('ネットワーク設定完了')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7omS-S1ddvxM"
      },
      "source": [
        "# 損失関数と最適化手法を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrUrUl-DdvxN"
      },
      "source": [
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
        "\n",
        "# 最適化手法の設定\n",
        "learning_rate = 2e-5\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3_6FHldvxQ"
      },
      "source": [
        "# 学習・検証を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKi0qSUMdvxR"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書オブジェクト\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # mask作成\n",
        "                    input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "                    input_mask = (inputs != input_pad)\n",
        "\n",
        "                    # Transformerに入力\n",
        "                    outputs, _, _ = net(inputs, input_mask)\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測（dim=1 列方向のＭａｘを取得、predsは最大のindex）\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()   #損失の計算\n",
        "                        optimizer.step()  # 勾配の更新\n",
        "\n",
        "                    # 結果の計算\n",
        "                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxT3CMEgdvxT"
      },
      "source": [
        "# 学習・検証を実行\n",
        "num_epochs = 14\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7RasnlwdvxX"
      },
      "source": [
        "torch.save(net_trained.state_dict(), './data/14_steps_fastText_weight.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzvLAH2Jdvxc"
      },
      "source": [
        "param = torch.load('./data/14_steps_fastText_weight.pth')\n",
        "net_trained.load_state_dict(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Iq4jEgdvxf"
      },
      "source": [
        "net_trained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuTmwgTEdvxi"
      },
      "source": [
        "# Attentionの可視化で判定根拠を探る\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH3124Iidvxj"
      },
      "source": [
        "# HTMLを作成する関数を実装\n",
        "\n",
        "\n",
        "def highlight(word, attn):\n",
        "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
        "\n",
        "    html_color = '#%02X%02X%02X' % (\n",
        "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
        "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
        "\n",
        "\n",
        "def mk_html(index, batch, preds, normlized_weights_1, normlized_weights_2, TEXT):\n",
        "    \"HTMLデータを作成する\"\n",
        "\n",
        "    # indexの結果を抽出\n",
        "    sentence = batch.Text[0][index]  # 文章\n",
        "    print(\"sentenceの形状：\", sentence.shape)\n",
        "    label = batch.Label[index]  # ラベル\n",
        "    print(\"labelの形状:\", label)\n",
        "    pred = preds[index]  # 予測\n",
        "    print(\"pored:\",pred.shape)\n",
        "    print(\"sentence:\", sentence)\n",
        "    print(label)\n",
        "\n",
        "    # indexのAttentionを抽出と規格化\n",
        "    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens1 /= attens1.max()\n",
        "\n",
        "    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens2 /= attens2.max()\n",
        "\n",
        "    # ラベルと予測結果を文字に置き換え\n",
        "    if label == 0:\n",
        "        label_str = \"Negative\"\n",
        "    else:\n",
        "        label_str = \"Positive\"\n",
        "\n",
        "    if pred == 0:\n",
        "        pred_str = \"Negative\"\n",
        "    else:\n",
        "        pred_str = \"Positive\"\n",
        "\n",
        "    # 表示用のHTMLを作成する\n",
        "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
        "\n",
        "    # 1段目のAttention\n",
        "    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens1):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    # 2段目のAttention\n",
        "    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens2):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    return html\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NboIH8advxo"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Transformerで処理\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)\n",
        "\n",
        "# ミニバッチの用意\n",
        "batch = next(iter(val_dl))\n",
        "\n",
        "# GPUが使えるならGPUにデータを送る\n",
        "inputs = batch.Text[0].to(device)  # 文章\n",
        "labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "print(\"inputs.shape=\",inputs.shape)\n",
        "# mask作成\n",
        "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "input_mask = (inputs != input_pad)\n",
        "print(\"input_mask.shape=\",input_mask.shape)\n",
        "#print(inputs)\n",
        "print(input_mask[0])\n",
        "# Transformerに入力\n",
        "outputs, normlized_weights_1, normlized_weights_2 = net_trained(\n",
        "    inputs, input_mask)\n",
        "_, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "\n",
        "index = 7 # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, normlized_weights_1,\n",
        "                      normlized_weights_2, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI9Ki9VHdvxr"
      },
      "source": [
        "# 推論用の1文章をインプットしてラベルとAttentionを可視化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZudSFkugdvxr"
      },
      "source": [
        "def preprocessing_text(text):\n",
        "    \n",
        "    # 半角・全角の統一\n",
        "    text = mojimoji.han_to_zen(text) \n",
        "    # 改行、半角スペース、全角スペースを削除\n",
        "    text = re.sub('\\r', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('　', '', text)\n",
        "    text = re.sub(' ', '', text)\n",
        "    text = re.sub('（','', text)\n",
        "    text = re.sub('）','', text)\n",
        "    # 数字文字の一律「0」化\n",
        "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
        "\n",
        "    # カンマ、ピリオド以外の記号をスペースに置換\n",
        "    for p in string.punctuation:\n",
        "        if (p == \".\") or (p == \",\"):\n",
        "            continue\n",
        "        else:\n",
        "            text = text.replace(p, \" \")\n",
        "\n",
        "    return text\n",
        "\n",
        "# 分かち書き\n",
        "def tokenizer_mecab(text):\n",
        "    m_t = MeCab.Tagger('-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
        "    text = m_t.parse(text)  # これでスペースで単語が区切られる\n",
        "    ret = text.strip().split()  # スペース部分で区切ったリストに変換\n",
        "    return ret\n",
        "\n",
        "# 前処理と分かち書きをまとめた関数を定義\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)  # 前処理の正規化\n",
        "    ret = tokenizer_mecab(text)  # Janomeの単語分割\n",
        "\n",
        "    return ret\n",
        "\n",
        "def create_tensor(text, max_length):\n",
        "    #入力文章をTorch Teonsor型にのINDEXデータに変換\n",
        "    token_ids = torch.ones((max_length)).to(torch.int64)\n",
        "    ids_list = list(map(lambda x: TEXT.vocab.stoi[x] , text))\n",
        "    print(ids_list)\n",
        "    for i, index in enumerate(ids_list):\n",
        "        token_ids[i] = index\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "# HTMLを作成する関数を実装\n",
        "\n",
        "\n",
        "def highlight(word, attn):\n",
        "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
        "\n",
        "    html_color = '#%02X%02X%02X' % (\n",
        "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
        "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
        "\n",
        "\n",
        "def mk_html(input, preds, normlized_weights_1, normlized_weights_2, TEXT):\n",
        "    \"HTMLデータを作成する\"\n",
        "\n",
        "    # indexの結果を抽出\n",
        "    index = 0\n",
        "    sentence = input.squeeze_(0) # 文章  #  torch.Size([1, 256])  > torch.Size([256]) \n",
        "    pred = preds[0]  # 予測\n",
        "\n",
        "\n",
        "    # indexのAttentionを抽出と規格化\n",
        "    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens1 /= attens1.max()\n",
        "\n",
        "    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens2 /= attens2.max()\n",
        "\n",
        "    if pred == 0:\n",
        "        pred_str = \"Negative\"\n",
        "    else:\n",
        "        pred_str = \"Positive\"\n",
        "\n",
        "    # 表示用のHTMLを作成する\n",
        "    html = '推論ラベル：{}<br><br>'.format(pred_str)\n",
        "  \n",
        "    # 1段目のAttention\n",
        "    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens1):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    # 2段目のAttention\n",
        "    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens2):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    return html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgMzXCyMdvxu"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "from dataloader_ja import *   ####\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)\n",
        "\n",
        "#インプットデータ\n",
        "text = \"課金売上に関しては、ユーザー数の増加により順調に推移した為、医科セグメントとしては、初の黒字化を達成する事が出来ました\"\n",
        "#textの先頭と末尾に<cls>、<eos>を追加する。\n",
        "text = tokenizer_with_preprocessing(text)\n",
        "text.insert(0, '<cls>')\n",
        "text.append('<eos>')\n",
        "#   '<cls>': 2, '<eos>': 3,\n",
        "text = create_tensor(text, 256)\n",
        "text = text.unsqueeze_(0)   #  torch.Size([256])  > torch.Size([1, 256])\n",
        "\n",
        "# GPUが使えるならGPUにデータを送る\n",
        "input = text.to(device)\n",
        "print(\"input_shape=\",input.shape)\n",
        "# mask作成\n",
        "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "input_mask = (input != input_pad)\n",
        "#print(input)\n",
        "#print(input_mask)\n",
        "\n",
        "outputs, normlized_weights_1, normlized_weights_2 = net_trained(input, input_mask)\n",
        "_, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "html_output = mk_html(input, preds, normlized_weights_1, normlized_weights_2, TEXT)  # HTML作成\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jvPusQzdvxy"
      },
      "source": [
        "display(HTML(html_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2QXN6sjdvx4"
      },
      "source": [
        "以上"
      ]
    }
  ]
}